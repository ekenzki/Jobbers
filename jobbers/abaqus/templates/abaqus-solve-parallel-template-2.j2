#!/bin/bash
#SBATCH -t {{job.timelimit}}                # timelimit in minutes
#SBATCH -J {{job.jobname}}           # the job name
#SBATCH --mail-type=END      # an email is send at the end of the job
# #SBATCH -n {{job.cpus}}                 # processors to use
#SBATCH --ntasks-per-node {{job.ntasks_per_node}}
#SBATCH -N {{job.nodes}}                 # Scania uses 1-3 nodes for parallel jobs
#SBATCH --lic {{job.abaqus_licenses.license}}:{{job.abaqus_licenses.volume}}         # licenses needed
#SBATCH --partition {{job.partitions|join(",")}}   # which partitions to consider

# SLURM_TASKS_PER_NODE
# Number of tasks to be initiated on each node. Values are comma separated and in the  same  order  as
# SLURM_JOB_NODELIST.  If two or more consecutive nodes are to have the same task count, that count is
# followed by "(x#)" where "#" is the repetition count.  For  example,  "SLURM_TASKS_PER_NODE=2(x3),1"
# indicates  that the first three nodes will each execute three tasks and the fourth node will execute
# one task.

# To execute the parallel direct sparse solver on computer clusters,
# the environment variable mp_host_list must be set to a list of host machines
# (see “Using the Abaqus environment settings,” Section 3.3.1).
# MPI-based parallelization is used between the machines in the host list.
# If more than one processor is available on a machine in the host list,
# thread-based parallelization is used within that host machine.
# For example, if the environment file has the following:
# cpus=8
# mp_host_list=[['maple',4],['pine',4]]
# Abaqus/Standard will use four processors on each host through thread-based parallelization.
# A total of two MPI processes (equal to the number of hosts)
# will be run across the host machines so that all eight processors are used by the parallel direct sparse solver.
# Input File Usage:
# Enter the following input on the command line:
# abaqus job=job-name cpus=n
# For example, the following input will run the job “beam” on two processors:
# abaqus job=beam cpus=2 


workdir=$SLURM_SUBMIT_DIR
scratch={{job.scratch}}
mkdir -p {{job.scratch}}

echo working dir: $workdir
echo scratch dir: $scratch
echo "SLURM_NODELIST: $SLURM_NODELIST"

PROJECT="abaqus-solve-parallel"                 # Project is the name of the script
INPUT=$(basename {{job.inpfile}})               # The inpfile parameter 

# Copy INPUT into SCRATCH, if it is newer than the one in SCRATCH #TODO: Needs to be fixed
if [[ ! -r "$scratch/$INPUT" || "$INPUT" -nt "$scratch/$INPUT" ]] ; then
   cp "$INPUT" $scratch
fi

# Version of Abaqus
module load {{job.abaqus_module.module}}
unset SLURM_GTIDS            # this is odd, but it has to be (!)

# Go to scratch
cd $scratch

# The number of processors per node
#ncpus=24
ncpus=`echo $SLURM_TASKS_PER_NODE | cut -c1-2`

# The number of total processors 
ntcpus=`expr $SLURM_NNODES \* $ncpus`

# The file of environment variables 
envFile=abaqus_v6.env

# Write the environment variables
echo "import os" > $envFile
echo "os.environ['ABA_BATCH_OVERRIDE'] = '1'" >> $envFile
echo "os.environ['MPI_REMSH'] = 'ssh'" >> $envFile
echo "verbose=3" >> $envFile

# Make lnx86_64.env (Needed?)
mpiEnv=lnx86_64.env
echo "import os" > $mpiEnv
echo "os.environ['ABA_BATCH_OVERRIDE'] = '1'" >> $mpiEnv
echo "os.environ['MPI_REMSH'] = 'ssh'" >> $mpiEnv


# The file of node lists
node_list=ABAQUS_NODES
 
echo "ncpus=$ncpus"

mp_host_list="["
for i in ${SLURM_JOB_NODELIST//,/ } ; do
    mp_host_list="${mp_host_list}['$i', $ncpus],"
done
mp_host_list=`echo ${mp_host_list} | sed -e "s/,$//"`
mp_host_list="${mp_host_list}]"

export mp_host_list

echo "mp_host_list=${mp_host_list}"  >> $envFile

# Run abaqus
ABQLauncher job=$INPUT cpus=$ntcpus -verbose 3 standard_parallel=all mp_mode=mpi interactive
